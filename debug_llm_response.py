#!/usr/bin/env python3
"""
–û—Ç–ª–∞–¥–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏ Ollama
"""

import asyncio
from interactive_chat import InteractiveMCPChat


async def debug_llm_response():
    """–û—Ç–ª–∞–¥–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏"""
    print("üîç === –û–¢–õ–ê–î–ö–ê –û–¢–í–ï–¢–û–í –ú–û–î–ï–õ–ò ===")
    print()
    
    chat = InteractiveMCPChat()
    
    try:
        # –ó–∞–ø—É—Å–∫ MCP —Å–µ—Ä–≤–µ—Ä–∞
        await chat.mcp_client.start_server()
        await chat.mcp_client.initialize()
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
        tools = await chat.mcp_client.list_tools()
        tools_for_llm = [{"name": tool["name"], "description": tool["description"], "parameters": tool["inputSchema"]} for tool in tools]
        
        print("‚úÖ MCP —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω")
        print()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Ollama
        if not chat.ollama.check_ollama_availability():
            print("‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —Ç–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–∞—Ä—Å–∏–Ω–≥")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å mock –æ—Ç–≤–µ—Ç–æ–º
            mock_response = """–î–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å—Ç—Ä–µ—á–∏ –≤–æ—Å–ø–æ–ª—å–∑—É—é—Å—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º:

[TOOL_CALL:schedule_meeting:{"date":"2024-01-15","time":"13:00","title":"–¢–µ—Å—Ç–æ–≤–∞—è –≤—Å—Ç—Ä–µ—á–∞"}]

–í—Å—Ç—Ä–µ—á–∞ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∞."""
            
            print(f"üìù MOCK –û–¢–í–ï–¢ –ú–û–î–ï–õ–ò:")
            print(f"   {mock_response}")
            print()
            
            import re
            tool_pattern = r'\[TOOL_CALL:([^:]+):([^\]]+)\]'
            tool_calls = re.findall(tool_pattern, mock_response)
            
            print(f"üîß –ù–ê–ô–î–ï–ù–ù–´–ï –í–´–ó–û–í–´ –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í: {len(tool_calls)}")
            for i, (tool_name, params) in enumerate(tool_calls, 1):
                print(f"   {i}. {tool_name} —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {params}")
            
            if tool_calls:
                print()
                print("üîÑ –¢–ï–°–¢–ò–†–£–ï–ú –û–ë–†–ê–ë–û–¢–ö–£:")
                final_response = await chat.process_llm_response(mock_response)
                print(f"üìã –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢:")
                print(f"   {final_response}")
        else:
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å —Ä–µ–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é
            print("‚úÖ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω - —Ç–µ—Å—Ç–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å")
            
            system_prompt = chat.build_system_prompt_with_tools(tools_for_llm)
            question = "–ó–∞–ø–ª–∞–Ω–∏—Ä—É–π –≤—Å—Ç—Ä–µ—á—É –Ω–∞ –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫ –Ω–∞ 13:00"
            full_prompt = f"{system_prompt}\n\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {question}\n\n–ü–æ–º–æ—â–Ω–∏–∫:"
            
            print("ü§ñ –û–¢–ü–†–ê–í–õ–Ø–ï–ú –ó–ê–ü–†–û–° –í OLLAMA...")
            print("üìã –°–ò–°–¢–ï–ú–ù–´–ô –ü–†–û–ú–ü–¢ (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):")
            print(f"   {system_prompt[:200]}...")
            print()
            
            response = chat.ollama.query_ollama(full_prompt)
            
            print(f"üì§ –û–¢–í–ï–¢ –ú–û–î–ï–õ–ò:")
            print(f"   {response}")
            print()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ä—Å–∏–Ω–≥
            import re
            tool_pattern = r'\[TOOL_CALL:([^:]+):([^\]]+)\]'
            tool_calls = re.findall(tool_pattern, response)
            
            print(f"üîß –ù–ê–ô–î–ï–ù–ù–´–ï –í–´–ó–û–í–´ –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í: {len(tool_calls)}")
            if tool_calls:
                for i, (tool_name, params) in enumerate(tool_calls, 1):
                    print(f"   {i}. {tool_name} —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {params}")
            else:
                print("   ‚ùå –ù–ï –ù–ê–ô–î–ï–ù–û –≤—ã–∑–æ–≤–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [TOOL_CALL:name:params]")
                print("   üí° –í–æ–∑–º–æ–∂–Ω–æ, –º–æ–¥–µ–ª—å –Ω–µ —Å–ª–µ–¥—É–µ—Ç –∑–∞–¥–∞–Ω–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    finally:
        await chat.mcp_client.stop_server()


if __name__ == "__main__":
    asyncio.run(debug_llm_response()) 